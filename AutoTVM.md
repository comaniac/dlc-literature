#tuning #schedule-template #simulated-annealing #transfer-learning #tree-gru #cpu #gpu

## Learning to Optimize Tensor Programs
## Bibtex
```
@inproceedings{chen2018learning,
  title={Learning to optimize tensor programs},
  author={Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3389--3400},
  year={2018}
}
```

## Paper
https://papers.nips.cc/paper/2018/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf

## Summary
- The auto-tuning framework based on [[TVM]].
- Search for the best parameter values in a predefined schedule template.
- Mainly rely on on-device measurements to determine the schedule quality.
- Use simulated annealing to generate the next schedule candidate.
- Use XGBoost as the cost model to identify a small set of schedule candidates from a large set of schedules generated by simulated annealing.
- Breakdown a program to tensors and use GBT and TreeGRU to encode tensor ASTs.
- The encoded tensor ASTs are used as feature to transfer the cost model learned from this program to other programs.

## Comments
N/A